import gzip
import json
import os
import argparse

def load_dataset(path, dataset_name):
    """åŠ è½½æ•°æ®é›†å¹¶å¤„ç†å¯èƒ½çš„æ ¼å¼é—®é¢˜"""
    print(f"Loading {dataset_name} from {path}...")
    if not os.path.exists(path):
        print(f"âš ï¸  Warning: {path} not found, skipping {dataset_name}")
        return None
    
    with gzip.open(path, 'rt') as f:
        data = json.load(f)
    
    if 'episodes' not in data:
        print(f"âš ï¸  Warning: {dataset_name} has no 'episodes' key, skipping")
        return None
    
    print(f"âœ“ Loaded {len(data['episodes'])} episodes from {dataset_name}")
    return data

def process_episodes(episodes, prefix, dataset_name):
    """ç»Ÿä¸€å¤„ç† episodes æ ¼å¼"""
    processed = []
    
    for ep in episodes:
        # ç¡®ä¿ episode_id å”¯ä¸€
        ep['episode_id'] = f"{prefix}_{ep['episode_id']}"
        
        # ç¡®ä¿æœ‰ trajectory_id
        if 'trajectory_id' not in ep:
            ep['trajectory_id'] = ep['episode_id']
        
        # ç»Ÿä¸€ instruction æ ¼å¼
        if 'instruction' in ep and isinstance(ep['instruction'], dict):
            instruction = ep['instruction']
            ep['instruction'] = {
                'instruction_text': instruction.get('instruction_text', ''),
                'instruction_tokens': instruction.get('instruction_tokens', [])
            }
        
        processed.append(ep)
    
    return processed

def merge_datasets(r2r_path=None, rxr_path=None, envdrop_path=None, scalevln_path=None, output_path=None):
    """
    åˆå¹¶å¤šä¸ª VLN æ•°æ®é›†
    
    Args:
        r2r_path: R2R æ•°æ®é›†è·¯å¾„
        rxr_path: RxR æ•°æ®é›†è·¯å¾„
        envdrop_path: EnvDrop æ•°æ®é›†è·¯å¾„
        scalevln_path: ScaleVLN æ•°æ®é›†è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    merged_episodes = []
    instruction_vocab = {}
    
    # åŠ è½½å¹¶å¤„ç†å„ä¸ªæ•°æ®é›†
    datasets = [
        (r2r_path, "r2r", "R2R"),
        (rxr_path, "rxr", "RxR"),
        (envdrop_path, "envdrop", "EnvDrop"),
        (scalevln_path, "scalevln", "ScaleVLN")
    ]
    
    for path, prefix, name in datasets:
        if path is None:
            continue
        
        data = load_dataset(path, name)
        if data is None:
            continue
        
        # å¤„ç† episodes
        processed = process_episodes(data['episodes'], prefix, name)
        merged_episodes.extend(processed)
        
        # ä¿ç•™ç¬¬ä¸€ä¸ªæ•°æ®é›†çš„ instruction_vocabï¼ˆå¦‚æœæœ‰ï¼‰
        if not instruction_vocab and 'instruction_vocab' in data:
            instruction_vocab = data['instruction_vocab']
    
    if not merged_episodes:
        print("âŒ Error: No episodes found in any dataset!")
        return
    
    # åˆ›å»ºåˆå¹¶æ•°æ®
    merged_data = {
        'episodes': merged_episodes,
        'instruction_vocab': instruction_vocab
    }
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š Total episodes: {len(merged_episodes)}")
    print(f"{'='*60}")
    
    # ä¿å­˜
    print(f"\nğŸ’¾ Saving to {output_path}...")
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with gzip.open(output_path, 'wt') as f:
        json.dump(merged_data, f)
    
    print("âœ… Done!")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="åˆå¹¶å¤šä¸ª VLN æ•°æ®é›†")
    
    # æ–¹æ¡ˆä¸€ï¼šæ ‡å‡†å››æ•°æ®é›†æ•´åˆï¼ˆæ¨èï¼‰
    parser.add_argument("--r2r", type=str, 
                        default="data/datasets/r2r/train/train.json.gz",
                        help="R2R è®­ç»ƒæ•°æ®è·¯å¾„")
    parser.add_argument("--rxr", type=str, 
                        default="data/datasets/rxr/train/train_follower_en.json.gz",
                        help="RxR è®­ç»ƒæ•°æ®è·¯å¾„ï¼ˆå»ºè®®ç”¨ train_follower_en.json.gzï¼‰")
    parser.add_argument("--envdrop", type=str, 
                        default="data/datasets/envdrop/train/train.json.gz",
                        help="EnvDrop è®­ç»ƒæ•°æ®è·¯å¾„ï¼ˆtrain.json.gz æˆ– envdrop/envdrop.json.gzï¼‰")
    parser.add_argument("--scalevln", type=str, 
                        default="data/datasets/scalevln/scalevln_subset_150k.json.gz",
                        help="ScaleVLN è®­ç»ƒæ•°æ®è·¯å¾„")
    parser.add_argument("--output", type=str, 
                        default="data/datasets/merged_train/train/train.json.gz",
                        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    # å¯é€‰ï¼šä¸åˆå¹¶æŸä¸ªæ•°æ®é›†ï¼ˆè®¾ä¸º Noneï¼‰
    parser.add_argument("--skip-rxr", action="store_true", help="è·³è¿‡ RxR æ•°æ®é›†")
    parser.add_argument("--skip-envdrop", action="store_true", help="è·³è¿‡ EnvDrop æ•°æ®é›†")
    parser.add_argument("--skip-scalevln", action="store_true", help="è·³è¿‡ ScaleVLN æ•°æ®é›†")
    
    args = parser.parse_args()
    
    # åº”ç”¨è·³è¿‡é€‰é¡¹
    rxr_path = None if args.skip_rxr else args.rxr
    envdrop_path = None if args.skip_envdrop else args.envdrop
    scalevln_path = None if args.skip_scalevln else args.scalevln
    
    print("ğŸš€ å¼€å§‹åˆå¹¶æ•°æ®é›†...")
    print(f"æ–¹æ¡ˆé…ç½®:")
    print(f"  - R2R: {args.r2r}")
    print(f"  - RxR: {rxr_path or '(è·³è¿‡)'}")
    print(f"  - EnvDrop: {envdrop_path or '(è·³è¿‡)'}")
    print(f"  - ScaleVLN: {scalevln_path or '(è·³è¿‡)'}")
    print(f"  - Output: {args.output}\n")
    
    merge_datasets(
        r2r_path=args.r2r,
        rxr_path=rxr_path,
        envdrop_path=envdrop_path,
        scalevln_path=scalevln_path,
        output_path=args.output
    )

# BudVLN Training Guide

æœ¬æŒ‡å—æä¾› BudVLN è®­ç»ƒçš„å®Œæ•´æµç¨‹è¯´æ˜ã€‚

## ğŸ¯ è®­ç»ƒæµç¨‹æ¦‚è§ˆ

```
1. ç¯å¢ƒå‡†å¤‡ â†’ 2. æ•°æ®å‡†å¤‡ â†’ 3. æ¨¡å‹è®­ç»ƒ â†’ 4. æ¨¡å‹è¯„ä¼° â†’ 5. éƒ¨ç½²
```

---

## 1ï¸âƒ£ ç¯å¢ƒå‡†å¤‡

### ç³»ç»Ÿè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Linuxï¼ˆæ¨èUbuntu 20.04+ï¼‰
- **GPU**: NVIDIA V100/A100ï¼ˆ40GBæ˜¾å­˜ï¼‰
- **CUDA**: 11.7+
- **Python**: 3.9+
- **å­˜å‚¨ç©ºé—´**: è‡³å°‘500GB

### å®‰è£…ä¾èµ–

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/lzy2208929036/BudVLN.git
cd BudVLN

# å®‰è£…Pythonä¾èµ–
pip install -r opensource_training/requirements.txt

# å®‰è£…Habitat-Lab
cd habitat-lab
pip install -e .
cd ..
```

### é…ç½®WandBï¼ˆå¯é€‰ä½†æ¨èï¼‰

```bash
# ç™»å½•WandB
wandb login

# æˆ–è®¾ç½®ç¯å¢ƒå˜é‡
export WANDB_API_KEY="your_api_key"
```

---

## 2ï¸âƒ£ æ•°æ®å‡†å¤‡

### ä¸‹è½½æ•°æ®é›†

```bash
# ä¸‹è½½Matterport3Dåœºæ™¯æ•°æ®
# è¯·è®¿é—®: https://niessner.github.io/Matterport/

# ä¸‹è½½VLNæ•°æ®é›†
mkdir -p data/datasets
cd data/datasets

# R2Ræ•°æ®é›†
wget https://www.dropbox.com/s/.../R2R_train.json
wget https://www.dropbox.com/s/.../R2R_val_seen.json
wget https://www.dropbox.com/s/.../R2R_val_unseen.json

# RxRæ•°æ®é›†
wget https://storage.googleapis.com/rxr-datasets/rxr_marky_train_guide.jsonl.gz
# ä¸‹è½½å…¶ä»–RxRæ•°æ®...
```

### åˆå¹¶æ•°æ®é›†

```bash
cd opensource_training/scripts
python merge_r2r_rxr_envdrop_scalevln.py \
    --output_dir ../../data/merged_datasets
```

### ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹

```bash
# ä»Hugging Faceä¸‹è½½
mkdir -p checkpoints
cd checkpoints

# ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
# å…·ä½“ä¸‹è½½æ–¹å¼è¯·å‚è€ƒä¸»README
```

---

## 3ï¸âƒ£ æ¨¡å‹è®­ç»ƒ

### æ–¹æ¡ˆA: æ··åˆè®­ç»ƒï¼ˆæ¨èï¼‰

**æœ€ä½³æ€§èƒ½ï¼Œæ”¶æ•›æœ€å¿«**

```bash
cd opensource_training/scripts
bash train_hybrid.sh
```

**è®­ç»ƒç‰¹ç‚¹ï¼š**
- ç»“åˆGRPOï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰å’ŒSFTï¼ˆç›‘ç£å­¦ä¹ ï¼‰
- åŠ¨æ€æƒé‡è¡°å‡ï¼šSFTæƒé‡ä»1.0é€æ¸é™åˆ°0.9
- ä¸“å®¶å¹²é¢„æœºåˆ¶è‡ªåŠ¨çº æ­£åç¦»è½¨è¿¹
- è®­ç»ƒæ—¶é—´ï¼šçº¦3-4å¤©ï¼ˆå•GPU A100ï¼‰

**å…³é”®å‚æ•°è°ƒæ•´ï¼š**

```bash
# åœ¨ train_hybrid.sh ä¸­ä¿®æ”¹ï¼š

# åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼ˆç‰ºç‰²æ€§èƒ½ï¼‰
--num_updates 300
--group_size 1

# æé«˜æ€§èƒ½ï¼ˆå¢åŠ è®­ç»ƒæ—¶é—´ï¼‰
--num_updates 800
--group_size 3
--sft_loss_start_weight 1.5
```

### æ–¹æ¡ˆB: çº¯GRPOè®­ç»ƒ

**ç”¨äºæ¶ˆèç ”ç©¶**

```bash
bash train_grpo.sh
```

### æ–¹æ¡ˆC: ä¸¤é˜¶æ®µSFTè®­ç»ƒ

**æ›´ç¨³å®šä½†å¯èƒ½æ€§èƒ½ç•¥ä½**

```bash
bash train_sft_twophase_merged.sh
```

### æ–­ç‚¹æ¢å¤

å¦‚æœè®­ç»ƒä¸­æ–­ï¼Œå¯ä»¥ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼š

```bash
# 1. ç¼–è¾‘ train_hybrid_resume.sh
# ä¿®æ”¹: RESUME_CHECKPOINT="result/your_checkpoint/checkpoint_XXX"

# 2. è¿è¡Œæ¢å¤è„šæœ¬
bash train_hybrid_resume.sh
```

---

## 4ï¸âƒ£ è®­ç»ƒç›‘æ§

### å®æ—¶æ—¥å¿—

```bash
# æŸ¥çœ‹å®æ—¶è®­ç»ƒæ—¥å¿—
tail -f result/grpo_hybrid_trainingV10_multi_dataset/training.log

# æŸ¥çœ‹å…³é”®æŒ‡æ ‡
grep "Update.*SR:" result/*/training.log
```

### WandBå¯è§†åŒ–

è®­ç»ƒä¼šè‡ªåŠ¨ä¸Šä¼ åˆ°WandBï¼š

1. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ä¸­çš„WandBé“¾æ¥ï¼š
```bash
grep "View run at" result/*/training.log
```

2. å…³é”®æŒ‡æ ‡ï¼š
   - `train/success_rate`: æˆåŠŸç‡
   - `train/spl`: SPLæŒ‡æ ‡
   - `train/oracle_rate`: ä¸“å®¶å¹²é¢„ç‡
   - `train/grpo_loss`: GRPOæŸå¤±
   - `train/sft_loss`: SFTæŸå¤±
   - `train/sft_weight`: å½“å‰SFTæƒé‡

### æ£€æŸ¥ç‚¹ç®¡ç†

```bash
# åˆ—å‡ºæ‰€æœ‰æ£€æŸ¥ç‚¹
ls -lh result/your_output_dir/checkpoint_*

# æŸ¥çœ‹æœ€æ–°æ£€æŸ¥ç‚¹
ls -t result/your_output_dir/checkpoint_* | head -1
```

---

## 5ï¸âƒ£ æ¨¡å‹è¯„ä¼°

### å•GPUè¯„ä¼°

```bash
python streamvln/streamvln_eval.py \
    --model_path result/your_checkpoint/checkpoint_XXX \
    --habitat_config_path config/vln_r2r_rxr.yaml \
    --split val_unseen
```

### å¤šGPUè¯„ä¼°ï¼ˆæ¨èï¼‰

```bash
cd opensource_training/scripts

# ç¼–è¾‘ streamvln_eval_multi_gpu.sh è®¾ç½®ï¼š
# - CHECKPOINT_PATH
# - NUM_GPUS

bash streamvln_eval_multi_gpu.sh
```

### è¯„ä¼°ç»“æœ

è¯„ä¼°å®Œæˆåä¼šç”Ÿæˆï¼š
- `eval_results.json`: è¯¦ç»†ç»“æœ
- æ§åˆ¶å°è¾“å‡ºå…³é”®æŒ‡æ ‡ï¼š
  - Success Rate (SR)
  - Success weighted by Path Length (SPL)
  - Oracle Success Rate (OSR)
  - Navigation Error (NE)

---

## ğŸ“ è®­ç»ƒæœ€ä½³å®è·µ

### 1. è®­ç»ƒç­–ç•¥

**ç¬¬ä¸€é˜¶æ®µï¼ˆå‰100-200æ¬¡æ›´æ–°ï¼‰ï¼š**
- é«˜SFTæƒé‡ï¼ˆ1.0-1.5ï¼‰
- å¼ºä¸“å®¶å¹²é¢„ï¼ˆdist_thresh=3.0ï¼‰
- è®©æ¨¡å‹å¿«é€Ÿå­¦ä¹ åŸºæœ¬å¯¼èˆª

**ç¬¬äºŒé˜¶æ®µï¼ˆ200-500æ¬¡æ›´æ–°ï¼‰ï¼š**
- é™ä½SFTæƒé‡ï¼ˆ0.5-0.9ï¼‰
- è®©GRPOä¸»å¯¼å­¦ä¹ 
- æ¨¡å‹è‡ªä¸»æ¢ç´¢

**ç¬¬ä¸‰é˜¶æ®µï¼ˆ500+æ¬¡æ›´æ–°ï¼‰ï¼š**
- ç¨³å®šSFTæƒé‡ï¼ˆ0.5-0.8ï¼‰
- æ€§èƒ½å¹³å°æœŸ
- é€‰æ‹©æœ€ä½³æ£€æŸ¥ç‚¹

### 2. è¶…å‚æ•°è°ƒä¼˜

**å­¦ä¹ ç‡ï¼š**
- ä» 5e-7 å¼€å§‹
- å¦‚æœä¸æ”¶æ•›ï¼Œé™åˆ° 1e-7
- å¦‚æœæ”¶æ•›å¤ªæ…¢ï¼Œæåˆ° 1e-6

**SFTæƒé‡è¡°å‡ï¼š**
- å¿«é€Ÿå®éªŒï¼š100-200 updates
- æ ‡å‡†è®­ç»ƒï¼š400-600 updates
- ç¨³å®šè®­ç»ƒï¼š800+ updates

**ä¸“å®¶å¹²é¢„å¼ºåº¦ï¼š**
```bash
# å¼ºå¹²é¢„ï¼ˆæ–°æ‰‹æ¨¡å‹ï¼‰
--offtrack_dist_thresh 2.0
--offtrack_patience 5

# ä¸­ç­‰å¹²é¢„ï¼ˆæ ‡å‡†ï¼‰
--offtrack_dist_thresh 3.0
--offtrack_patience 8

# å¼±å¹²é¢„ï¼ˆæˆç†Ÿæ¨¡å‹ï¼‰
--offtrack_dist_thresh 5.0
--offtrack_patience 12
```

### 3. æ•°æ®é›†é€‰æ‹©

| é…ç½®æ–‡ä»¶ | æ•°æ®é›† | è®­ç»ƒé€Ÿåº¦ | æ€§èƒ½ | æ¨èåœºæ™¯ |
|---------|--------|---------|------|---------|
| `vln_r2r.yaml` | R2R | å¿« | åŸºå‡† | å¿«é€ŸéªŒè¯ |
| `vln_r2r_rxr.yaml` | R2R+RxR | ä¸­ | æ›´å¥½ | æ ‡å‡†è®­ç»ƒ |
| `vln_merged_standard.yaml` | 4ä¸ªæ•°æ®é›† | æ…¢ | æœ€ä½³ | å®Œæ•´è®­ç»ƒ |
| `vln_merged_fast.yaml` | é‡‡æ ·ç‰ˆ | ä¸­ | å¥½ | å¹³è¡¡æ–¹æ¡ˆ |

---

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜1: CUDA Out of Memory

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ–¹æ³•1: é™ä½æ‰¹é‡å¤§å°
--mini_batch_size 1
--group_size 1

# æ–¹æ³•2: ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
--gradient_accumulation_steps 2

# æ–¹æ³•3: å¯ç”¨DeepSpeed ZeRO-2
--use_deepspeed
--deepspeed_config scripts/zero2.json
```

### é—®é¢˜2: è®­ç»ƒä¸æ”¶æ•›

**æ£€æŸ¥æ¸…å•ï¼š**
1. âœ… SFTæƒé‡æ˜¯å¦è¶³å¤Ÿé«˜ï¼ˆstart_weight >= 1.0ï¼‰
2. âœ… ä¸“å®¶å¹²é¢„æ˜¯å¦æ­£å¸¸è§¦å‘ï¼ˆoracle_rate 10-30%ï¼‰
3. âœ… å­¦ä¹ ç‡æ˜¯å¦åˆé€‚ï¼ˆå°è¯•5e-7ï¼‰
4. âœ… æ•°æ®é›†æ˜¯å¦æ­£ç¡®åŠ è½½

**è°ƒè¯•å‘½ä»¤ï¼š**
```bash
# æ£€æŸ¥ä¸“å®¶å¹²é¢„ç‡
grep "Oracle Rate" result/*/training.log

# æ£€æŸ¥æŸå¤±æ›²çº¿
grep "GRPO Loss\|SFT Loss" result/*/training.log | tail -20
```

### é—®é¢˜3: ä¸“å®¶å¹²é¢„ç‡è¿‡é«˜ï¼ˆ>50%ï¼‰

**å¯èƒ½åŸå› ï¼š**
- é˜ˆå€¼è®¾ç½®è¿‡ä¸¥
- æ¨¡å‹è´¨é‡å·®

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ”¾å®½é˜ˆå€¼
--offtrack_dist_thresh 4.0
--offtrack_patience 12

# æˆ–å…ˆè¿›è¡ŒSFTé¢„è®­ç»ƒ
bash train_sft_twophase_merged.sh
```

### é—®é¢˜4: ä¸“å®¶å¹²é¢„ç‡è¿‡ä½ï¼ˆ<5%ï¼‰

**å¯èƒ½åŸå› ï¼š**
- é˜ˆå€¼è®¾ç½®è¿‡æ¾
- æ¨¡å‹å·²ç»å¾ˆå¥½ï¼ˆä¸æ˜¯é—®é¢˜ï¼‰

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ”¶ç´§é˜ˆå€¼
--offtrack_dist_thresh 2.5
--offtrack_patience 5
```

---

## ğŸ“¦ è®­ç»ƒè¾“å‡º

æ¯æ¬¡è®­ç»ƒä¼šç”Ÿæˆï¼š

```
result/your_output_dir/
â”œâ”€â”€ training.log              # å®Œæ•´è®­ç»ƒæ—¥å¿—
â”œâ”€â”€ checkpoint_10/            # æ¯10æ¬¡æ›´æ–°ä¿å­˜
â”œâ”€â”€ checkpoint_20/
â”œâ”€â”€ ...
â””â”€â”€ checkpoint_XXX/          # æœ€ä½³æ£€æŸ¥ç‚¹
    â”œâ”€â”€ adapter_config.json
    â”œâ”€â”€ adapter_model.bin    # LoRAæƒé‡
    â””â”€â”€ trainer_state.json
```

---

## ğŸš€ è¿›é˜¶ä¸»é¢˜

### è‡ªå®šä¹‰æ•°æ®é›†

1. å‡†å¤‡æ•°æ®æ ¼å¼ï¼ˆå‚è€ƒR2Ræ ¼å¼ï¼‰
2. åˆ›å»ºHabitaté…ç½®æ–‡ä»¶
3. ä¿®æ”¹è®­ç»ƒè„šæœ¬ä¸­çš„ `HABITAT_CONFIG`

### å¤šå¡è®­ç»ƒ

```bash
# ä½¿ç”¨DeepSpeed
deepspeed --num_gpus=4 streamvln/streamvln_grpo_train.py \
    --use_deepspeed \
    --deepspeed_config scripts/zero3.json \
    # å…¶ä»–å‚æ•°...
```

### è®­ç»ƒç›‘æ§è„šæœ¬

```bash
# åˆ›å»ºç›‘æ§è„šæœ¬
cat > watch_training.sh << 'EOF'
#!/bin/bash
while true; do
    clear
    echo "=== Latest Training Progress ==="
    tail -50 result/your_output_dir/training.log | grep -E "Update|SR:|SPL:"
    echo ""
    nvidia-smi --query-gpu=utilization.gpu,memory.used --format=csv
    sleep 60
done
EOF

chmod +x watch_training.sh
./watch_training.sh
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [PARAMETERS_EXPLAINED.md](PARAMETERS_EXPLAINED.md) - å‚æ•°è¯¦è§£
- [../README.md](../README.md) - ä¸»README
- [../../README.md](../../README.md) - é¡¹ç›®README

---

## ğŸ’¬ è·å–å¸®åŠ©

- **GitHub Issues**: æäº¤bugæŠ¥å‘Šæˆ–åŠŸèƒ½è¯·æ±‚
- **è®¨è®º**: GitHub Discussions
- **é‚®ä»¶**: è”ç³»è®ºæ–‡ä½œè€…

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸ‰**
